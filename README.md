##### Sources
https://docs.scipy.org/doc/

https://www.tensorflow.org/api_docs/python/

http://tflearn.org/

http://neuralnetworksanddeeplearning.com/

https://github.com/mnielsen/neural-networks-and-deep-learning

https://deepnotes.io/implementing-cnn

https://github.com/parasdahal/deepnet

https://www.youtube.com/watch?v=aircAruvnKk

https://www.youtube.com/watch?v=IHZwWFHWa-w

https://www.youtube.com/watch?v=Ilg3gGewQ5U

https://www.youtube.com/watch?v=tIeHLnjs5U8

https://sudeepraja.github.io/Neural/

http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/

https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/

https://github.com/wiseodd/hipsternet

https://wiseodd.github.io/techblog/2016/07/16/convnet-conv-layer/

http://cs231n.github.io/convolutional-networks/

http://ruder.io/optimizing-gradient-descent/

https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f

https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c

https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/batch_norm_layer.html

https://en.wikipedia.org/wiki/Activation_function

https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html

https://wiseodd.github.io/techblog/2016/07/04/batchnorm/

