\documentclass[12pt,a4paper]{report}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[onehalfspacing]{setspace}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{pifont}

\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage[hidelinks]{hyperref}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{tabto}
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows,calc}
\usepackage{pgfplots}
\usepackage{amsmath}

\begin{document}

\begin{titlepage}
	\centering
	{\Large Gymnasium Bäumlihof, 5Bb \par}
	\vspace{1cm}
	{\LARGE\scshape Maturaarbeit\par}
	\vspace{1.5cm}
	{\huge\bfseries Kann der Computer Werbung erkennen?\par}
	\vspace{0.6cm}
    {\Large Bilderkennung mit einem Neuronalen Netzwerk\par}
	\vspace{2cm}
	{\Large\itshape Georg Schwan\par}
	\vfill
	Betreuungsperson\par
	{\itshape Test1\par}
	Korreferent\par
	{\itshape Test2}
	\vfill
	{\large Datum\par}
\end{titlepage}

\tableofcontents

\newpage

\chapter{Einleitung}\label{ch:einleitung}

\section{Motivation}
\label{sec:motivation}

\section{Aufbau der Arbeit}
\label{sec:aufbauDerArbeit}

\chapter{Neuronales Netzwerk}
\label{ch:neuronalesNetzwerk}

\section{Konzept}\label{sec:konzept}
Wenn man ein normales Programm schreiben will muss man das Problem in viele kleiner aufteilen, bis der Computer fähig ist
es zu lösen.
In einem Neuronale Netzwerk sagen wir dem Computer nicht wie er das Problem lösen kann, sondern ein Neuronales Netzwerk
versteht das Problem, indem wir es Beispieldaten geben und es daran lernen kann, bis es seine eigene Lösung gefunden hat.
Zum Beispiel, wir wollen einem Netzwerk beibringen ob in einem Bild ein Auto vorkommt,
dazu geben wir dem Neuronale Netzwerk viele Bilder, mit und ohne Auto.
Mit jedem Bild, dass das Neuronale Netzwerk bekommt, lernt es besser wie ein Auto ausschaut.

Das Konzept eines Neuronales Netzwerk ist nicht etwas neuses.
Im Jahre 1957 hat Frank Rosenblatt ein erste Idee eines Neuronales Netzwerk vorgestellt, aber erst in den letzten
Jahren ist der grosse Hype des Neuronale Netzwerkes ausgebrochen, dies liegt daran, dass man früher nicht die Daten
und Rechenleistung hatte, wie sie heute zur verfügung steht.

\section{Neuron}\label{sec:neuron}
User Gehirn kann Entscheidungen treffe, da wir billionen von Neuronen haben, die miteinander verbunden sind und sich
verständigen können.
Aber ein Neuron an sich ist praktisch nutzlos, aber in grosser Anzahl können sie komplexeste Probleme lösen.

Nach dem gleichen Prinzip funktioniert ein Neuronales Netzwerk
Es besteht aus vielen Neuronen (daher der Name) die miteinander verbunden sind.


\begin{figure}[!h]
    \centering
\begin{tikzpicture}[
init/.style={
  draw,
  circle,
  inner sep=2pt,
  font=\Huge,
  join = by -latex
},
squa/.style={
  draw,
  inner sep=2pt,
  font=\Large,
  join = by -latex
},
start chain=2,node distance=13mm
]
\node[on chain=2]
  (x2) {$x_2$};
\node[on chain=2,join=by o-latex]
  {$w_2$};
\node[on chain=2,init] (sigma)
  {$\displaystyle\Sigma$};
\node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activate \\ function}},join=by -latex, minimum size=0.9cm]
  {$f$};
\node[on chain=2,label=above:Ausgabe,join=by -latex]
  {$y$};
\begin{scope}[start chain=1]
\node[on chain=1] at (0,1.5cm)
  (x1) {$x_1$};
\node[on chain=1,join=by o-latex]
  (w1) {$w_1$};
\end{scope}
\begin{scope}[start chain=3]
\node[on chain=3] at (0,-1.5cm)
  (x3) {$x_3$};
\node[on chain=3,label=below:Gewichte,join=by o-latex]
  (w3) {$w_3$};
\end{scope}
\node[label=above:\parbox{2cm}{\centering Verzerrung \\ $b$}] at (sigma|-w1) (b) {};

\draw[-latex] (w1) -- (sigma);
\draw[-latex] (w3) -- (sigma);
\draw[o-latex] (b) -- (sigma);

\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Eingabe} (x3.south west);
\end{tikzpicture}
    \caption{Einzelner Neuron in einem Neuronalen Netzwerks}
    \label{fig:neuron1}
\end{figure}

Ein Neuron in einem Neuronales Netzwerk wird als Mathematische funktion definiert wie Abbildung~\ref{fig:neuron1}
verdeutlicht.
Ein Neuron hat $n$ verschiedene Eingaben, die als $x_j$ bezeichnet werden und mit einem spezifischen Gewicht $w_j$ multipliziert werden.
Die Ausgabe erfolgt indem man alle gewichteten Eingaben, mit einem bias $b$, addiert und durch eine so genannte
Aktivierungsfunktion durchlaufen läst.
Als Formel:
\[y =f\left(\sum_{j=1}^{n} x_j w_j + b\right)\]
Die Gewichte $w_j$ und die Verzerrung $b$ des Neurons sind die Parameter, die angepasst werden und somit das Neuron lernfähig machen.

Quelle: https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f

Eine Aktivierungsfunktion ist nötig, da ohne eine wäre ein Neuronales Netzwerk eine komplett lineare funktion, welches
nur lineare Probleme lösen könnte.
Die meisten Probleme sind viel komplexer als das man sie linear darstellen könnte und deswegen ist eine aktivierungsfunktion von nötig.

Ein Beispiel für eine Aktivierungsfunktion ist die Sigmoidfunktion, wie sie auf Abbildung~\ref{fig:activation1} zu sehen ist.
\begin{figure}[h]
    \centering
\begin{tikzpicture}
\begin{axis}[
    axis lines = left,
    xlabel = $x$,
    ylabel = {$f(x)$},
]
\pgfplotsset{
every axis legend/.append style={
at={(0.1,0.9)},
anchor=north west,
},
}
\addplot [
    domain=-8:8,
    samples=200,
    color=blue,
    ]
    {1 / (1 + 2.71828^(-x))};
\addlegendentry{$\frac{1}{1 + e^{-x}}$}

\end{axis}
\end{tikzpicture}
    \caption{Sigmoidfunktion}
    \label{fig:activation1}
\end{figure}
Besonders an dieser Funktion ist, dass sie den Ausgabewert zwischen 0 und 1 eingrenzt, was uns erlaubt den Ausgabewert des ganzen
Netzwerkes besser zu deuten, als wenn der Wert von $-\infty$ bis $\infty$ gross sein könnte.

Jedes Neuron in einem Netzwerk kann eine andere Aktivierungsfunkton haben.

\section{Architektur}
Wie auch im biologischen Gehirn ist ein Neuron allein nutzlos.
Erst wenn man die Neuronen miteinander verbindet kann es komplexe Zusammenhänge modellieren.
\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering} & |[plain]| \parbox{1.3cm}{\centering} & |[plain]| \parbox{1.3cm}{\centering} \\
& |[plain]| \\
|[plain]| & \\
& |[plain]| \\
  |[plain]| & |[plain]| \\
& & \\
  |[plain]| & |[plain]| \\
& |[plain]| \\
  |[plain]| & \\
& |[plain]| \\    };
\foreach \ai [count=\mi ]in {2,4,...,10}
  \draw[<-] (mat-\ai-1) -- node[above] {Eingabe $x_\mi$} +(-2.8cm,0);
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,6,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,6,9}
  \draw[->] (mat-\ai-2) -- (mat-6-3);
\draw[->] (mat-6-3) -- node[above] {Ausgabe} +(2.5cm,0);
\end{tikzpicture}
    \caption{Mögliche architektur eines Neuronalen Netzwerk}
    \label{fig:network1}
\end{figure}

Eine mögliche Architektur kann wie in  Abbildung~\ref{fig:network1} ausschauen.
Ein Netzwerk wird generell immer in verschiedene Schichten unterteilt.
Die linke Schicht wird als eingabe Schicht bezeichnet und die Neuronen in dieser Schicht werden eingabe Neuronen genannt.
Analog da zu wird die rechte Schicht ausgabe Schicht genannt, die die ausgabe Neuronen beinhaltet.
Die mittleren Schichten, die von der Anzahl variieren können, werden versteckte Schichten genannt.
Die Anzahl der Neuronen in jeder Schicht kann auch variieren.
Abbildung~\ref{fig:network2} zeigt eine andere mögliche Architektur für ein Netzwerk, welches 2 versteckte
Schichten hat.
\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
& |[plain]|& |[plain]|& |[plain]| \\
|[plain]| && |[plain]|& |[plain]| \\
& |[plain]|&& |[plain]|& |[plain]| \\
  |[plain]| && |[plain]|&& |[plain]| \\
& |[plain]| & \\
  |[plain]| && |[plain]|&& |[plain]| \\
& |[plain]| && |[plain]|& |[plain]|  \\
  |[plain]| & & |[plain]|& |[plain]|  \\
& |[plain]|& |[plain]|& |[plain]| \\     };
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,5, 7,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,5,7,9}
{\foreach \aii in {4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {4,6,8}
{\foreach \aii in {5, 7}
  \draw[->] (mat-\ai-3) -- (mat-\aii-4);
}
\draw[decorate,decoration={brace}] (mat-1-1.west) -- node[above=5pt] {\parbox{3cm}{\centering Eingabe\\Schicht}} (mat-1-1.east);
\draw[decorate,decoration={brace}] (mat-1-2.west) -- node[above=5pt] {\parbox{3cm}{\centering Versteckte\\Schichten}} (mat-1-3.east);
\draw[decorate,decoration={brace}] (mat-1-4.west) -- node[above=5pt] {\parbox{3cm}{\centering Ausgabe\\Schicht}} (mat-1-4.east);
\draw[decorate,decoration={brace,mirror}] ($(mat-1-1.west)+(-0.1cm,-0.2cm)$) -- node[left=5pt] {\parbox{2cm}{\centering Eingabe\\Neuronen}} ($(mat-10-1.west) + (-0.1cm,-16pt)$);
\draw[decorate,decoration={brace}] ($(mat-5-4.east)+(+0.1cm,+16pt)$) -- node[right=5pt] {\parbox{2cm}{\centering Ausgabe\\Neuronen}} ($(mat-7-4.east) + (+0.1cm,-16pt)$);
\end{tikzpicture}
    \caption{Neuronales Netzwerk mit 2 versteckten Schichten}
    \label{fig:network2}
\end{figure}
Jeder Neuron von der vorigen Schicht ist mit jedem Neuron der nachfolgenden Schicht verbunden.
Dies ist ein klassisches vorwärtsgekoppeltes Netzwerk (im englischen feed forward network).
Wichtig zu beachten ist, dass keine Schleifen vorkommen.
Es gibt Architekturen in denen Schleifen vorkommen, aber auf diese wird nicht näher eingegangen, da sie für die
Bilderkennung nicht relevant sind.

\subsection{Formel}

Um eine allgemeine Formel bestimmen zu können, muss man zuerst die Elemente des Netzwerks benennen.
Wir bezeichnen das Gewicht $w^l_{k,j}$ für die Verbindung des $k^{ten}$ Neuron der $(l-1)^{ten}$ Schicht
zu dem $j^{ten}$ Neuron der $l^{ten}$ Schicht.
Ähnlich dazu bezeichnen wir die Ausgabe des Neurons als $a^l_j$ und der bias des Neurons als $b^l_j$,
Abbildung~\ref{fig:network3} verdeutlicht diese Notation.

\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{2cm}{\centering Schicht 1} & |[plain]| \parbox{2cm}{\centering Schicht 2} & |[plain]| \parbox{2cm}{\centering Schicht 3} \\
& |[plain]| \\
|[plain]| & \\
& |[plain]| \\
  |[plain]| & |[plain]| \\
& & \\
  |[plain]| & |[plain]| \\
& |[plain]| \\
  |[plain]| & \\
& |[plain]| \\    };
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,6,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2) coordinate[midway](center1\ai\aii);
}
\foreach \ai in {3,6,9}
  \draw[->] (mat-\ai-2) -- (mat-6-3) coordinate[midway](center2\ai);


\node[] at (mat-8-1)
  (x3) {$b^1_4$};
\node[] at (mat-3-2)
  (x3) {$a^2_1$};

\node (h1) at (-1.5, 2.2) {$w^2_{1,2}$};

\draw[->] (h1) -- (center126);

\node (h2) at (3, 1.5) {$w^3_{3,1}$};

\draw[->] (h2) -- (center29);

\end{tikzpicture}
    \caption{Bezeichnung der Parameter}
    \label{fig:network3}
\end{figure}
Mit dieser Notation kann eine Formel für das Netzwerk aufgestellt werden, welche sehr ähnlich zu der Formel vom Abschnitt~\ref{sec:neuron} ist.

\[a^l_j = f\left(\sum_{k} a^{l-1}_k w^l_{k,j} + b^l_j\right)\]

\section{Lernen}\label{sec:lernen}
Bis jetzt ging es nur darum wie ein Neuronales Netzwerk aufgebaut ist.
In dem Abschnitt geht wie ein Neuronales Netzwerk, anhand von Daten, lernen kann
\subsection{Kostenfunktion}
Damit ein Netzwerk lernen kann muss man dem Netzwerk zuerst sagen können wie gut oder wie schlecht es gerade ist.
Dazu definieren wir eine Kostenfunktion $C$, die von allen Gewichten $w$ und allen biases $b$ abhängig ist.
Das Netzwerk wird als Funktion $y(x)$ bezeichnet.
Den Eingabewert wird als $x$ bezeichnet mit dem dazugehörige Label $l$.
Beachte, dass $x$ und $l$ Vektoren sind.
Zum Beispiel würde ein Bild, das 10x10 Pixel gross ist, einen ($10 * 10 = 100$) 100-dimensionaler Vektor bekommen
und die Label, wenn es 3 verschiedene Ergebnisse gibt, einem 3-dimensional Vektor, der z.B so ausschaut: $(0, 1, 0)$

\[C(w,b) = \left|y(x)-l\right|^2\]

Das Ziel des Netzwerkes ist diese Kostenfunktion zu minimieren, bis idealerweise $C \approx 0$,
dies geschieht wenn die Ausgabe des Netzwerks und des Label sehr ähnlich sind.
\subsection{Gradient Descent}
Um diese Kostenfunktion zu minimieren wird ein Algorithmus names Gradient Descent benutzt.
Das Konzept basiert darauf, dass man eine Funktion, in abhängigkeit einer Variablen, ableiten kann und so die Steigung
and diesem Punkt berechnen kann und die Variable richtung Minimum anpasst.

**Bild Einfügen**

Zum Beispiel hat man eine Kostenfunktion $C(a,b)$ die von $a$ und $b$ abhängig ist.
Wenn man diese Graphisch abbildet, sieht es wie auf Abbildung **ref**.

Die Variablen $a$ und $b$ werden am Anfang zufällige Werte zugeteilt und das Ziel ist sie so anzupassen,
dass man das Minimum der Kostenfunktion findet.
Um das Minimum zu finden kann man sich einen Ball vorstellen, der in das Minimum herunterrollt.
Um dies zu berechnen muss man die Steigung, mithilfe einer Ableitung, herausfinden und die Variable in die
gegensätzliche Richtung bewegen, wie auf Abbildung **ref** zu sehen ist.

**Bild Einfügen**

Die Bewegung als Formel sieht so aus:
\begin{gather*}
    a \rightarrow a^\prime\ = a - \mu\frac{\partial C}{\partial a}\\
    b \rightarrow b^\prime\ = b - \mu\frac{\partial C}{\partial b}\\
\end{gather*}
wobei $\mu$ eine kleine positive Zahl (learning rate genannt) ist, die die Geschwindigkeit der Bewegung steuert.
Ausserdem beachte, dass der Ball keine Beschleunigung hat.
Wenn man diese Formel nun immer wieder anwendend gelangt man zum Minimum der Kostenfunktion.

Der Algorithmus funktioniert auch bei mehr als nur 2 Variablen und sieht fürs Neuronale Netzwerk identisch aus.
\begin{gather*}
    w^l_{k,j} \rightarrow w^{l^\prime}_{k,j} = w^l_{k,j} - \mu\frac{\partial C}{\partial w^l_{k,j}}\\
    b^l_j \rightarrow b^{l^\prime}_j = b^l_j - \mu\frac{\partial C}{\partial b^l_j}\\
\end{gather*}

\subsection{Backpropagation}



\chapter{Lösungsansatz}
\label{ch:lösungsansatz}

\chapter{Umsetzung}
\label{ch:umsetzung}

\chapter{Reflexion}
\label{ch:reflexion}

%\clearpage
%\phantomsection
%\addcontentsline{toc}{chapter}{Literaturverzeichnis}
%\nocite{*}
%\bibliographystyle{plain}
%\bibliography{literatur}
%
%\clearpage
%\phantomsection
\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}

\listoffigures

\appendix

\chapter*{Ehrlichkeitserklärung}

Die eingereichte Arbeit ist das Resultat meiner persönlichen, selbstständigen Beschäftigung mit dem Thema.
Ich habe für sie keine anderen Quellen benutzt als die in den Verzeichnissen aufgeführten.
Sämtliche wörtlich übernommenen Texte (Sätze) sind als Zitate gekennzeichnet.

\vspace{2cm}
Insert Datum
\end{document}
