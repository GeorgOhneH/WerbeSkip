\documentclass[12pt,a4paper]{report}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[onehalfspacing]{setspace}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{pifont}

\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage[hidelinks]{hyperref}
\usepackage{wrapfig}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{multirow}
\usepackage{url}
\usepackage{pdfpages}
\usepackage{tabto}
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows,calc}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{subfig}

\graphicspath{{C:/Jetbrains/PyCharm/WerbeSkip/thoughts/arbeit/}}

\begin{document}

\begin{titlepage}
	\centering
	{\Large Gymnasium Bäumlihof, 5Bb \par}
	\vspace{1cm}
	{\LARGE\scshape Maturaarbeit\par}
	\vspace{1.5cm}
	{\huge\bfseries Kann der Computer Werbung erkennen?\par}
	\vspace{0.6cm}
    {\Large Bilderkennung mit einem Neuronalen Netzwerk\par}
	\vspace{2cm}
	{\Large\itshape Georg Schwan\par}
	\vfill
	Betreuungsperson\par
	{\itshape Test1\par}
	Korreferent\par
	{\itshape Test2}
	\vfill
	{\large \today\par}
\end{titlepage}

\tableofcontents

\newpage

\chapter{Einleitung}\label{ch:einleitung}

\section{Motivation}
\label{sec:motivation}
Vor ein paar Jahren haben wir Fernsehen geschaut und immer wenn Werbung kam haben wir den Sender gewechselt,
bis die Werbung vorbei war und das normale Programm weiter lief.
Das Problem war nur, dass wir nie wussten wann die Werbung vorbei war.
Meinem Bruder ist aufgefallen, dass bei Werbung nie das Logo vom Sender eingespielt wird.
Daraufhin hat er probiert ein Algorithmus zu schreiben, der das Logo von einem Sender erkennen kann.
Er versuchte das Logo mithilfe von Bedingungen und Schleifen auszudrücken, aber vergebens.

Als ich auf der Suche nach einer Idee für eine Maturaarbeit war erinnerte ich mich wieder an das Problem und an einen neuen Lösungsansatz,
nämlich Neuronale Netzwerke, welche Heute überall verwendet werden und extrem Mächtig sind.
Die Idee war aber nicht nur ein Logo zu erkennen sondern auch genau verstehen wie ein neuronales Netzwerk funktioniert und warum es so Mächtig ist.
\section{Aufbau der Arbeit}
\label{sec:aufbauDerArbeit}

\chapter{Problemstellung}
Das Ziel dieser Arbeit ist einen Algorithmus zu programmieren der Bilder als Werbung erkennen kann.
Dafür wird ein neuronales Netzwerk benutzt, das sich auf die Bilderkennung beschränkt.
\section{Logo}
Wie schon gesagt wird bei Werbung das Senderlogo nicht eingeblendet und deswegen kann das Problem
vereinfacht werden auf die Frage ob das Senderlogo eingeblendet ist oder nicht (siehe Abbildung~\ref{fig:logo1}).
\begin{figure}[h]%
    \centering
    \subfloat[Werbung]{{\includegraphics[width=7cm]{assets/images/prosieben_kein_logo.png} }}%
    \qquad
    \subfloat[Keine Werbung]{{\includegraphics[width=7cm]{assets/images/prosieben_logo.png} }}%
    \caption{Kein Senderlogo bei Werbung}%
    \label{fig:logo1}%
\end{figure}
Man könnte meinen, dass das erkennen eines Logo's relativ simple ist.
Zum Beispiel könnte man Schauen ob der Bereich, wo das Logo sein sollte, heller ist als ausserhalb.

\begin{figure}[h]%
    \centering
    \subfloat[]{{\includegraphics[width=3cm]{assets/images/logo_beispiel1.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{assets/images/logo_beispiel3.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{assets/images/logo_beispiel4.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3cm]{assets/images/logo_beispiel5.png} }}%
    \caption{Logo mit verschiedenen Hintergründen}%
    \label{fig:logo2}%
\end{figure}

Ein Problem des Logo's ist, dass es nicht einfach über das normale Bild eingespielt wird, sondern man kann leicht hindurch sehen (siehe Abbildung~\ref{fig:logo2}a),
dadurch kann man das Logo nicht an gleichen Pixel erkennen.
Die grösste Schwierigkeit ist aber, dass bei manchen Hintergründen, vor allem bei Weissen, das Logo abgeschnitten oder kaum bis gar nicht sichtbar ist.
Bei Abbildung~\ref{fig:logo2} (b) ist das Logo abgeschnitten, bei (c) ist es kaum sichtbar und bei (d) ist komplett verschwunden.

Ein andere Schwierigkeit ist, dass das Logo nicht unbedingt immer am gleichen Ort sein muss.
Zum Beispiel hat Prosieben 3 verschiedene Positionen (siehe Abbildung~\ref{fig:logo3}).
\begin{figure}[h]%
    \centering
    \subfloat[]{{\includegraphics[width=6cm]{assets/images/logo_boarder_above.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=6cm]{assets/images/logo_boarder_side.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=6cm]{assets/images/logo_no_boarder.png} }}%
    \caption{Logo an verschiedenen Positionen}%
    \label{fig:logo3}%
\end{figure}
Alle diese Schwierigkeiten machen das erkennen eines Logo's ohne ein neuronales Netzwerk extrem schwer.
Ein neuronales Netzwerk hingegen löst das Probleme ziemlich elegant.

Eine weiter interessante Frage auf die eingegangen wird ist, ob ein neuronales Netzwerk auch ohne ein Logo Werbung erkennen kann.

\section{Neuronales Netzwerk}
Neuronale Netzwerke sind ein sehr umfangreiches Thema und deswegen begrenze ich mich auf Netzwerke die für die Bilderkennung entscheidend sind.
Darunter sind klassische feedforward und convolution Netzwerke.
Auf recurrent neuronale Netzwerke\footnote{Ein neuronales Netzwerk, das geeignet für Sequenzen ist} wird nicht näher eingegangen.


\chapter{Neuronales Netzwerk}
\label{ch:neuronalesNetzwerk}
Dieses ganze Kapitel bezieht sich auf das Buch von Michael A. Nielsen\cite{neuralbook}, ausser es ist anders angegeben.
\section{Konzept}\label{sec:konzept}
Wenn man ein normales Programm schreiben will muss man das Problem in viele kleinere aufteilen, bis der Computer fähig ist,
es zu lösen.
In einem Neuronale Netzwerk wird dem Computer nicht gesagt wie es das Problem lösen kann, sondern ein neuronales Netzwerk
versteht das Problem, indem es beispiel Daten bekommt und an ihnen lernen kann, bis es seine eigene Lösung gefunden hat.
Zum Beispiel, wir wollen einem Netzwerk beibringen ob in einem Bild ein Auto vorkommt,
dazu geben wir dem neuronalem Netzwerk viele Bilder, mit und ohne Auto.
Mit jedem Bild, dass das neuronale Netzwerk bekommt, lernt es besser wie ein Auto ausschaut.

Das Konzept eines neuronales Netzwerk ist nicht etwas Neues.
Im Jahre 1957 hat Frank Rosenblatt ein erste Idee eines neuronales Netzwerk vorgestellt.
Die Idee war aus mathematischen Funktionen unser Gehirn zu modellieren.
Indem man die biologischen Neuronen und Synapsen als mathematische Funktion ausdrückt.

Es ist aber erst in den letzen Jahren ist der grosse Hype für neuronale Netzwerke ausgebrochen,
dies liegt daran, dass man erst jetzt die nötigen Daten
und Rechenleistung zu verfügung hat.

\section{Neuron}\label{sec:neuron}
User Gehirn kann Entscheidungen treffen, da wir Billionen von Neuronen haben, die miteinander verbunden sind und sich
verständigen können.
Ein Neuron an sich ist praktisch nutzlos, aber in grosser Anzahl können sie komplexeste Probleme lösen.

Nach dem gleichen Prinzip funktioniert ein neuronales Netzwerk,
Es besteht aus vielen Neuronen (daher der Name) die miteinander verbunden sind.


\begin{figure}[h]
    \centering
\begin{tikzpicture}[
init/.style={
  draw,
  circle,
  inner sep=2pt,
  font=\Huge,
  join = by -latex
},
squa/.style={
  draw,
  inner sep=2pt,
  font=\Large,
  join = by -latex
},
start chain=2,node distance=13mm
]
\node[on chain=2]
  (x2) {$x_2$};
\node[on chain=2,join=by o-latex]
  {$w_2$};
\node[on chain=2,init] (sigma)
  {$\displaystyle\Sigma$};
\node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Aktivierungsfunktion}},join=by -latex, minimum size=0.9cm]
  {$f$};
\node[on chain=2,label=above:Ausgabe,join=by -latex]
  {$y$};
\begin{scope}[start chain=1]
\node[on chain=1] at (0,1.5cm)
  (x1) {$x_1$};
\node[on chain=1,join=by o-latex]
  (w1) {$w_1$};
\end{scope}
\begin{scope}[start chain=3]
\node[on chain=3] at (0,-1.5cm)
  (x3) {$x_3$};
\node[on chain=3,label=below:Gewichte,join=by o-latex]
  (w3) {$w_3$};
\end{scope}
\node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};

\draw[-latex] (w1) -- (sigma);
\draw[-latex] (w3) -- (sigma);
\draw[o-latex] (b) -- (sigma);

\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Eingabe} (x3.south west);
\end{tikzpicture}
    \caption{Einzelner Neuron in einem Neuronalen Netzwerks}
    \label{fig:neuron1}
\end{figure}

Ein Neuron in einem neuronales Netzwerk wird als mathematische Funktion definiert wie Abbildung~\ref{fig:neuron1}
verdeutlicht.
Ein Neuron hat $n$ verschiedene Eingaben, die als $x_j$ bezeichnet werden und mit einem spezifischen Gewicht $w_j$ multipliziert werden.
Die Ausgabe erfolgt, indem man alle gewichteten Eingaben, mit einem Bias $b$, addiert und durch eine so genannte
Aktivierungsfunktion $f$ durchlaufen läst.
Eine klassische Aktivierungsfunktion ist die Sigmoid Funktion $f(x) = \frac{1}{1 + e^{-x}}$, welche den Wert zwischen 0 und 1 normalisiert.
Als Gleichung:
\[y =f\left(\sum_{j=1}^{n} x_j w_j + b\right)\]
Die Gewichte $w_j$ und der Bias $b$ des Neurons sind die Parameter, die angepasst werden und somit das Neuron lernfähig machen.

Eine Aktivierungsfunktion ist nötig, da ohne eine wäre ein neuronales Netzwerk eine komplett lineare funktion, welches
nur lineare Probleme lösen könnte\cite{activations}, da in einem Netzwerk nur multipliziert und addiert wird.
Durch die Aktivierungsfunktion kommt eine nicht lineare Funktion hinzu, welche das Netzwerk komplizierter machen,
aber auch mächtiger, da es so Verbindungen lernen kann, die man nicht vorher nicht herstellen konnte.
Ohne diese Aktivierungsfunktion wäre das Netzwerk nicht in der Lage komplizierte zusammen hänge wie auf Bilder oder Sprache zu erkennen.\cite{activations}
Es wird näher auf die Aktivierungsfunktion eingegangen im Abschnitt~\ref{sec:aktivierungsfunktionen}

\section{Architektur}
Wie auch im biologischen Gehirn ist ein Neuron allein nutzlos.
Erst wenn man die Neuronen miteinander verbindet kann es komplexe Zusammenhänge modellieren.
\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering} & |[plain]| \parbox{1.3cm}{\centering} & |[plain]| \parbox{1.3cm}{\centering} \\
& |[plain]| \\
|[plain]| & \\
& |[plain]| \\
  |[plain]| & |[plain]| \\
& & \\
  |[plain]| & |[plain]| \\
& |[plain]| \\
  |[plain]| & \\
& |[plain]| \\    };
\foreach \ai [count=\mi ]in {2,4,...,10}
  \draw[<-] (mat-\ai-1) -- node[above] {Eingabe $x_\mi$} +(-2.8cm,0);
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,6,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,6,9}
  \draw[->] (mat-\ai-2) -- (mat-6-3);
\draw[->] (mat-6-3) -- node[above] {Ausgabe} +(2.5cm,0);
\end{tikzpicture}
    \caption{Mögliche Architektur eines Neuronalen Netzwerk}
    \label{fig:network1}
\end{figure}

Eine mögliche Architektur kann wie in  Abbildung~\ref{fig:network1} ausschauen.
Ein Netzwerk wird generell immer in verschiedene Schichten unterteilt.
Die linke Schicht wird als eingabe Schicht bezeichnet und die Neuronen in dieser Schicht werden Eingabe Neuronen genannt.
Analog dazu wird die rechte Schicht Ausgabe Schicht genannt, die die Ausgabe Neuronen beinhaltet.
Die mittleren Schichten, die von der Anzahl her variieren können, werden versteckte Schichten genannt.
Die Anzahl der Neuronen in jeder Schicht kann auch variieren.
Abbildung~\ref{fig:network2} zeigt eine andere mögliche Architektur für ein Netzwerk, welches 2 versteckte
Schichten hat.
\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
& |[plain]|& |[plain]|& |[plain]| \\
|[plain]| && |[plain]|& |[plain]| \\
& |[plain]|&& |[plain]|& |[plain]| \\
  |[plain]| && |[plain]|&& |[plain]| \\
& |[plain]| & \\
  |[plain]| && |[plain]|&& |[plain]| \\
& |[plain]| && |[plain]|& |[plain]|  \\
  |[plain]| & & |[plain]|& |[plain]|  \\
& |[plain]|& |[plain]|& |[plain]| \\     };
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,5, 7,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,5,7,9}
{\foreach \aii in {4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {4,6,8}
{\foreach \aii in {5, 7}
  \draw[->] (mat-\ai-3) -- (mat-\aii-4);
}
\draw[decorate,decoration={brace}] (mat-1-1.west) -- node[above=5pt] {\parbox{3cm}{\centering Eingabe\\Schicht}} (mat-1-1.east);
\draw[decorate,decoration={brace}] (mat-1-2.west) -- node[above=5pt] {\parbox{3cm}{\centering Versteckte\\Schichten}} (mat-1-3.east);
\draw[decorate,decoration={brace}] (mat-1-4.west) -- node[above=5pt] {\parbox{3cm}{\centering Ausgabe\\Schicht}} (mat-1-4.east);
\draw[decorate,decoration={brace,mirror}] ($(mat-1-1.west)+(-0.1cm,-0.2cm)$) -- node[left=5pt] {\parbox{2cm}{\centering Eingabe\\Neuronen}} ($(mat-10-1.west) + (-0.1cm,-16pt)$);
\draw[decorate,decoration={brace}] ($(mat-5-4.east)+(+0.1cm,+16pt)$) -- node[right=5pt] {\parbox{2cm}{\centering Ausgabe\\Neuronen}} ($(mat-7-4.east) + (+0.1cm,-16pt)$);
\end{tikzpicture}
    \caption{Neuronales Netzwerk mit 2 versteckten Schichten}
    \label{fig:network2}
\end{figure}
Jedes Neuron der vorigen Schicht ist mit jedem Neuron der nachfolgenden Schicht verbunden,
welches als \textit{völlig verbundene Schicht} bezeichnet werden.
Dies ist ein klassisches feedforward Netzwerk,
welches nur Verbindungen nach vorne hat und so keine Schleifen entstehen können\footnote{Es gibt auch Architekturen in denen Schleifen vorkommen,
aber auf diese wird nicht näher eingegangen}.

\subsection{Beschriftung}

Um eine allgemeine Gleichung zu bestimmen, muss man zuerst die Elemente des Netzwerks benennen.
Wir bezeichnen das Gewicht $w^l_{k,j}$ für die Verbindung des $k^{ten}$ Neuron der $(l-1)^{ten}$ Schicht
zu dem $j^{ten}$ Neuron der $l^{ten}$ Schicht.
Ähnlich dazu bezeichnen wir die Ausgabe des Neurons als $a^l_j$ und der Bias des Neurons als $b^l_j$ (siehe Abbildung~\ref{fig:network3}).
\footnote{Das $l$ dient nur zur Indexierung und nicht als Potenz}

\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{2cm}{\centering Schicht 1} & |[plain]| \parbox{2cm}{\centering Schicht 2} & |[plain]| \parbox{2cm}{\centering Schicht 3} \\
& |[plain]| \\
|[plain]| \\
& &|[plain]| \\
  |[plain]| & |[plain]| \\
& & \\
  |[plain]| & |[plain]| \\
& &|[plain]| \\
  |[plain]| \\
& |[plain]| \\    };
\foreach \ai in {2,4,...,10}
{\foreach \aii in {4,6,8}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2) coordinate[midway](center1\ai\aii);
}
\foreach \ai in {4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-6-3) coordinate[midway](center2\ai);


\node[] at (mat-4-2)
  (x3) {$a^2_1$};
\node[] at (mat-6-2)
  (x3) {$a^2_2$};
\node[] at (mat-8-2)
  (x3) {$a^2_3$};
\node[] at (mat-6-3)
  (x3) {$a^3_1$};

\node (h1) at (-1.5, 2.2) {$w^2_{2,1}$};

\draw[->] (h1) -- (center144);

\node (h2) at (3, 1.5) {$w^3_{3,1}$};

\draw[->] (h2) -- (center28);

\end{tikzpicture}
    \caption{Bezeichnung der Parameter}
    \label{fig:network3}
\end{figure}
Mit dieser Notation kann eine Gleichung für das Netzwerk aufgestellt werden, welche der Gleichung einem Neuron ähnelt~\ref{sec:neuron}.

\[a^l_j = f\left(\sum_{k} a^{l-1}_k w^l_{k,j} + b^l_j\right)\]

\section{Wie das Netzwerk lernt}\label{sec:lernen}
Bis jetzt ging es nur darum wie ein neuronales Netzwerk aufgebaut ist.
In dem Abschnitt geht wie ein neuronales Netzwerk, anhand von Daten, lernen kann
\subsection{Kostenfunktion}
Damit ein Netzwerk lernen kann muss man dem Netzwerk zuerst sagen können wie gut oder wie schlecht es gerade ist.
Dazu definieren wir eine Kostenfunktion $C$, die von allen Gewichten $w$ und allen Biases $b$ abhängig ist.
Der Ausgabewert des kompletten Netzwerk wird als $y$ bezeichnet und die entsprechende gewünschte Ausgabe als $l$.
Beachte, dass $y$ und $l$ Vektoren sind.
Zum Beispiel würde ein Bild, das 10x10 Pixel gross ist, als einen ($10 * 10 =$)100-dimensionaler Vektor dargestellt werden,
wobei jeder Eintrag im Vektor der Grauwert eines Pixels ist.
Die Dimension vom ausgabe Vektor $y$ und des gewünschten ausgabe Vektor $l$ entspricht der Anzahl Neuronen in der letzten Schicht des Netzwerks,
wobei jedes Neuron etwas bestimmten Aussagt.
Zum Beispiel könnte ein Neuron für das vorkommen eines Auto, im Bild, stehen, wobei 0 für kein Auto und 1 für ein Auto steht.

\[C(w,b) = \sum_{j}(y_j - l_j)^2\]

Das Ziel des Netzwerkes ist diese Kostenfunktion zu minimieren, bis so viele beispiel Daten wie möglich $C \approx 0$ entsprechen,
dies geschieht wenn die Ausgabe des Netzwerks und die gewünschte Ausgabe ähnlich ist.
\subsection{Gradient Descent}
Um diese Kostenfunktion zu minimieren wird ein Algorithmus names \textit{gradient descent} benutzt.
Das Konzept basiert darauf, dass man eine Funktion, in Abhängigkeit einer Variablen, ableiten kann und so die Steigung (eng. gradient),
an diesem Punkt, berechnen kann und die Variable richtung Minimum anpasst.

\begin{figure}[h]%
    \centering
    \includegraphics[width=10cm]{C:/Jetbrains/PyCharm/WerbeSkip/thoughts/arbeit/assets/python/gradient_empty.pdf} %
    \caption{Kostenfunktion in abhängigkeit von $x$}%
    \label{fig:grad1}%
\end{figure}

Zum Beispiel hat man eine Kostenfunktion $C(x)$ die von $x$ abhängig ist (siehe Abbildung~\ref{fig:grad1}).

Die Variable $x$ wird am Anfang einen zufällige Werte zugewiesen,
welches dem Orangen Punkt auf der Abbildung~\ref{fig:grad1} entspricht und das Ziel ist $s$ so anzupassen,
dass man ein Minimum der Kostenfunktion findet.
Um ein Minimum zu finden kann man sich einen Ball vorstellen, der in ein Minimum herunterrollt.
Um dies zu berechnen muss man die Steigung, mithilfe einer Ableitung, herausfinden und die Variable in die
gegensätzliche Richtung bewegen.
\[a \rightarrow a^\prime\ = a - \eta\frac{\partial C}{\partial a}\]
wobei $\mu$ eine kleine positive Zahl (learning rate genannt) ist, die die Geschwindigkeit der Bewegung steuert.
Ausserdem beachte, dass der Ball keine Beschleunigung hat.
Wenn man diese Gleichung iterativ anwendend gelangt man früher oder später zum lokalen Minimum der Kostenfunktion (siehe Abbildung~\ref{fig:grad2}).

\begin{figure}[h]%
    \centering
    \includegraphics[width=10cm]{C:/Jetbrains/PyCharm/WerbeSkip/thoughts/arbeit/assets/python/gradient_full.pdf} %
    \caption{2-dimensionaler Verlauf des gradient descent}%
    \label{fig:grad2}%
\end{figure}

Der Algorithmus funktioniert auch bei mehr als nur einer Variable und lässt sich für die Gewichte und Biases des Netzwerkes genau gleich berechnen.
\begin{gather*}
    w^l_{k,j} \rightarrow w^{l^\prime}_{k,j} = w^l_{k,j} - \eta\frac{\partial C}{\partial w^l_{k,j}}\\
    b^l_j \rightarrow b^{l^\prime}_j = b^l_j - \eta\frac{\partial C}{\partial b^l_j}\\
\end{gather*}
Durch dieses Verfahren kann zwar relativ einfach ein Minimum gefunden werden, dabei ist aber zu beachten, dass es sich um ein lokales
Minimum handelt und kein globales.

\subsection{Backpropagation}
Der Algorithmus um $\frac{\partial C}{\partial w^l_{k,j}}$ und $\frac{\partial C}{\partial b^l_j}$ zu berechnen wird als
Backpropagation bezeichnet und ist der mathematisch schwerste Teil dieser Arbeit.
Es ist aber nicht unbedingt nötig für das Verständnis eines Neuronale Netzwerkes.
Es wird auch nicht näher auf die Beweise der Gleichungen eingegangen,
da es sonst komplizierter wird und im Grunde ist es nur die Anwendung der Kettenregel.

Um die Übersicht zu behalten wird eine Zwischenmenge $\delta^l_j$ eingeführt, welches als \textit{Fehler} bezeichnet wird.
Der Fehler sagt aus wie gut oder schlecht ein Neuron ist und ist definiert als:
\[\delta^l_j = \frac{\partial C}{\partial z^l_j}\]
wobei $z^l_j$ die Ausgabe von einem Neuron ohne die Aktivierungsfunktion ist, also $a^l_j = f(z^l_j)$.
Mit dieser Definition kann man den Fehler in der letzten Schicht $L$ bestimmen:
\[\delta^L_j = \frac{\partial C}{\partial a^L_j}f^\prime(z^L_j)\]
%Der linke Teil $\frac{\partial C}{\partial a^L_j}$ gibt an wie schnell sich die Kosten in Abhängigkeit des $j^{ten}$ ausgang Neuron ändern.
%Der rechte Teil $f^\prime(z^L_j)$ zeigt wie schnell sich die Aktivierungsfunktion $f$ ändert bei $z^L_j$
In unserem Fall benutzen wir eine quadratische Kostenfunktion $C = \sum_{j}(a^L_j - y_j)^2$ bei
der die Ableitung $\frac{\partial C}{\partial a^L_j} = 2(a^L_j - y_j)$ ist und können $\delta^L_j$ einfacher definieren als:
\[\delta^L_j = 2(a^L_j - y_j)f^\prime(z^L_j)\]
Bei der Berechnung des Fehlers $\delta^l_j$ Abhängig von $\delta^{l+1}_j$ bekommt man:
\[\delta^l_j = \sum_k w^{l+1}_{j,k}\delta^{l+1}_k f^\prime(z^l_j)\]
Beachte, dass bei $w^{l+1}_{j,k}$ das $j$ und $k$ vertauscht sind, so dass man durch alle Neuronen der $(l+1)^{ten}$ Schicht durch iteriert.
Mit dieser Gleichung kann man jeden Fehler von jeder Schicht berechnen, indem man von hinten durch das Netzwerk durchläuft.
Ähnlich wie wenn man sich beim Netzwerk nach vorne bewegt.

Die Gleichung für die Änderungsrate der Kosten in Bezug auf ein Bias im Netzwerk ist genau der Fehler:
\[\frac{\partial C}{\partial b^l_j} = \delta^l_j\]
Die Gleichung für die Änderungsrate der Kosten in Bezug auf ein Gewicht im Netzwerk:
\[\frac{\partial C}{\partial w^{l}_{k,j}} = a^{l-1}_k\delta^l_j\]

\section{Aktivierungsfunktionen}\label{sec:aktivierungsfunktionen}
Das einzige was noch fehlt ist wie eine Aktivierungsfunktionen genau ausschaut.
Wie schon gesagt darf eine Aktivierungsfunktionen nicht linear sein, da sie sonst nichts neues dem Netzwerk beiträgt.
\subsection{Sigmoid}
Ein Beispiel für eine Aktivierungsfunktion ist die Sigmoid Funktion $f(x) = \frac{1}{1 + e^{-x}}$ (siehe Abbildung~\ref{fig:activation1}).
\begin{figure}[h]
    \centering
\begin{tikzpicture}
\begin{axis}[
    axis lines = left,
    xlabel = $x$,
    ylabel = {$f(x)$},
    ymin = -0.1,
    ymax = 1.1,
]
\pgfplotsset{
every axis legend/.append style={
at={(0.1,0.9)},
anchor=north west,
},
}
\addplot [
    domain=-8:8,
    samples=200,
    color=blue,
    ]
    {1 / (1 + 2.71828^(-x))};
\addlegendentry{$\frac{1}{1 + e^{-x}}$}

\end{axis}
\end{tikzpicture}
    \caption{Sigmoid Aktivierungsfunktionen}
    \label{fig:activation1}
\end{figure}
Besonders an dieser Funktion ist, dass sie den Ausgabewert zwischen 0 und 1 eingrenzt, was uns erlaubt den Ausgabewert des ganzen
Netzwerkes besser zu deuten, als wenn der Wert zwischen $-\infty$ und $\infty$ liegt.
Ein Problem der Sigmoid Funktion ist, dass wenn die Ausgabe nah bei 1 oder 0 ist, dann ist die Ableitung $f^\prime(x)$ davon auch nah bei 0,
was den Fehler $\delta^l_j$ sehr klein hält und so das Netzwerk nur noch langsam lernen lässt.
Dieses Problem ist als \textit{vanishing gradient problem} bekannt.
\subsection{Rectified Linear Units}
Eine andere populäre Aktivierungsfunktion ist die Rectified linear units Funktion oder kurz ReLu.
\begin{figure}[h]
    \centering
\begin{tikzpicture}
\begin{axis}[
    axis lines = left,
    xlabel = $x$,
    ylabel = {$f(x)$},
    ymin = -2,
]
\pgfplotsset{
every axis legend/.append style={
at={(0.1,0.9)},
anchor=north west,
},
}
\addplot [
    domain=-8:8,
    samples=200,
    color=blue,
    ]
    {max(x, 0))};
\addlegendentry{$\max(0, x)$}

\end{axis}
\end{tikzpicture}
    \caption{ReLu Aktivierungsfunktionen}
    \label{fig:activation2}
\end{figure}
Die Funktion $f(x) = \max(0, x)$ (siehe Abbildung~\ref{fig:activation2}).
löst das Problem des vanishing gradient und praktisch alle Neuronalen Netzwerke benutzen Relu als ihre Aktivierungsfunktion,
da es die besten Ergebnisse erbringt.\cite{activations}
Ein Nachteil ist, dass sie nur in den Versteckten Schichten gut funktioniert,
da der Ausgabewert der Funktion unendlich gross sein kann.
\subsection{Softmax}
Die softmax Funktion wird verwendet, um eindeutige Klassifikationen zu machen und ist definiert als:
\[f(x_i) = \frac{e^{x_i}}{\sum_k{e^{z_k}}}\]
Das besondere an dieser Aktivierungsfunktion ist, dass sie nicht nur einen Wert braucht, sondern alle Werte der ganzen Schicht,
d.h nicht nur ein $x_i$ sondern alle.
Ausserdem gibt die Summe aller Resultate $\sum_i f(x_i) = 1$ und kann deswegen als eine Wahrscheinlichkeitsverteilung verstanden werden.
Dies ist oft sehr hilfreich, da viele Probleme nur ein richtiges Resultat haben, zum Beispiel hat man Bilder von Zahlen,
wo immer nur eine Zahl pro Bild zu sehen ist.
Und durch die softmax Funktion sieht man dann eine geschätzte Wahrscheinlichkeit vom Netzwerk für jede Zahl.
\section{Convolution}
Bis jetzt ging es nur um Schichten die völlig miteinander verbunden sind.
Für die Bilderkennung kann das suboptimal sein, da bestimmte Eigenschaften eines Bildes nicht miteinbezogen werden,
wie zum Beispiel die Beziehung von nebeneinander liegenden Pixel
und das gesuchte Objekt in einem Bild an verschiedenen Orten vorkommen kann.
\subsection{Architektur}
Die Eingabe für einen convolutional Schicht ist nicht 1-Dimensional, sondern 2-Dimensional (siehe Abbildung~\ref{fig:conv1}).
\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=5pt
    },
  nodes in empty cells,
  column sep=2pt,
  row sep=2pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|  & |[plain]| & |[plain]|\\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
&  &   &  &  &  &  &  &  & \\
};
\node[] at (mat-1-6.north west)
  (x) {\centering Eingabe Neuronen};
\end{tikzpicture}
    \caption{Eingabe Neuronen für eine convolution Schicht}
    \label{fig:conv1}
\end{figure}
Die Neuronen werden normal verbunden, einfach mit dem Unterschied, dass nicht jeder Neuron mit jedem Neuron verbunden wird,
sondern dass nur ein bestimmter Bereich zum nächsten Neuron verbunden ist.
Dieser Bereich wird als \textit{Filter} bezeichnet und in dem Beispiel auf Abbildung~\ref{fig:conv2} wird ein 3x3 Filter benutzt.
\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=5pt
    },
  nodes in empty cells,
  column sep=2pt,
  row sep=2pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|  & |[plain]| & |[plain]|\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
};
\node[] at (mat-1-6.north)
  (x) {\centering Eingabe Neuronen};

\node[] at (mat-5-15.north)
  (y) {\centering Verstecktes Neuron};

\foreach \ai in {7,...,9}
{\foreach \aii in {7,...,9}
  \node[circle, line width=1.5pt, draw, inner sep=5pt] at (mat-\ai-\aii) {};
}
\foreach \ai in {7,...,9}
{\foreach \aii in {7,...,9}
  \draw[->] (mat-\ai-\aii) -- (mat-6-15);
}
\end{tikzpicture}
    \caption{Verbindung eines versteckten Neurons in einem convolution Schicht}
    \label{fig:conv2}
\end{figure}
Der Filter wird dann auf den Eingabe Neuronen um ein Neuron verschoben, um den nächsten Neuron zu verbinden.
Und so geht das weiter, auch nach unten, bis die ganze versteckte Schicht gemacht wurde.
Dabei wird die versteckte Schicht auch kleiner, in dem Beispiel wird die 10x10 Schicht zu einer 8x8 Schicht,
da der Filter irgendwann am anderen Rand anstösst.
Abbildung~\ref{fig:conv3} verdeutlicht das Prinzip noch einmal.

Der Filter kann auch um mehr als nur einen Neuronen verschoben werden, und man kann in den beiden Richtungen verschiedene Werte nehmen,
zum Beispiel bewegt sich der Neuron nach links um zwei Neuronen und nach unten um drei.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=3pt
    },
  nodes in empty cells,
  column sep=1pt,
  row sep=1pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|  & |[plain]| & |[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|  & |[plain]| & |[plain]|& |[plain]|  & |[plain]|  & |[plain]|  & |[plain]|  & |[plain]|  \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
};
\node[] at (mat-1-6.north)
  (x) {\centering Eingabe Neuronen};

\node[] at (mat-1-18.north east)
  (y) {\centering Versteckte Schicht};

\foreach \ai in {2,...,4}
{\foreach \aii in {1,...,3}
  \node[circle, line width=1pt, draw, inner sep=3pt] at (mat-\ai-\aii) {};
}
\foreach \ai in {2,...,4}
{\foreach \aii in {1,...,3}
  \draw[->] (mat-\ai-\aii) -- (mat-3-15);
}

\end{tikzpicture}
    \hspace{0.5cm}% NO SPACE!
    \begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=3pt
    },
  nodes in empty cells,
  column sep=1pt,
  row sep=1pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|  & |[plain]| & |[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|  & |[plain]| & |[plain]|& |[plain]|  & |[plain]|  & |[plain]|  & |[plain]|  & |[plain]|  \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| &  &   &  &  &  &  & &\\
&  &   &  &  &  &  &  &  &  &|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| \\
};
\node[] at (mat-1-6.north)
  (x) {\centering Eingabe Neuronen};

\node[] at (mat-1-18.north east)
  (y) {\centering Versteckte Schicht};

\foreach \ai in {2,...,4}
{\foreach \aii in {2,...,4}
  \node[circle, line width=1pt, draw, inner sep=3pt] at (mat-\ai-\aii) {};
}
\foreach \ai in {2,...,4}
{\foreach \aii in {2,...,4}
  \draw[->] (mat-\ai-\aii) -- (mat-3-16);
}

\end{tikzpicture}
    \caption{Bewegung eines Filters über eine convolution Schicht}
    \label{fig:conv3}
\end{figure}

Das entscheidende am Filter ist, dass er die gleichen Gewichte und Bias verwendet für die Verbindung, d.h bei einem
Filter von 5x5 gibt is $(5*5=)$25 verschiedene Gewichte und einen Bias.
Wenn der Filter bewegt wird werden immer die gleichen Gewichte und der der gleiche Bias verwendet.
Als Gleichung bei einem 3x3 Filter:

\[a^{l+1}_{j,k} = f\left(\sum_{p=0}^{2}\sum_{m=0}^{2}w^l_{p,m}a^l_{j+p,k+m} + b^l\right)\]

wobei $a_{x, y}$ der Neuron an der Position $x, y$ ist und $f$ eine Aktivierungsfunktion.
Dadurch das immer die gleichen Gewichte und der gleiche Bias für jeden Filter benutzt werden,
wird überall das gleiche Merkmal erkannt auch wenn es sich an einem anderen Ort befindet,
deswegen wird die Ausgabe von dem Filter als \textit{feature map} bezeichnet.
Normalerweise will man mehr als nur ein Merkmal erkennen und deswegen werden mehrere Filter verwendet,
wodurch mehrere feature maps entstehen (siehe Abbildung~\ref{fig:conv4}).
Der Grund warum die Filter nicht alle das gleiche Merkmal erkennen, liegt an der zufälligen initialisierung der Gewichte und Biases

\begin{figure}[h]%
    \centering
\begin{tikzpicture}
\draw [draw=black] (5,5.5) rectangle (0,0.5);
\draw [draw=black] (12,5) rectangle (7,0);
\draw [draw=black] (12.5,5.5) rectangle (7.5,0.5);
\draw [draw=black] (13,6) rectangle (8,1);
\draw[->] (5, 3) -- (7, 2.5);
\draw[->] (5, 3) -- (7.5, 3);
\draw[->] (5, 3) -- (8, 3.5);
\node[] at (2.5, 6.5)
  (y) {\centering Eingabe Schicht};

\node[] at (10, 6.5)
  (y) {\centering Versteckte Schicht};

\end{tikzpicture}
    \caption{Convolution Schicht mit 3 Feature Maps}%
    \label{fig:conv4}
\end{figure}

Die feature map kann noch grösser gemacht werden, indem ein Rand von Neuronen, mit dem Wert 0, hinzugefügt wird.\cite{conv}
Das führt dazu, dass sich der Filter über die normale feature map hinweg bewegt, was in machen Fällen erwünscht ist.

\subsection{Max Pooling}
Neben den Convolution Schichten gibt es auch die maxpool Schicht\footnote{Es gibt noch andere pooling Schichten, die aber in dieser Arbeit nicht verwendet wurden},
die Idee vom Pooling ist, dass es die Informationen zusammenfasst.

max polling nimmt eine Feature Map als Eingabe und lässt auch etwas ähnliches wie ein Filter drüber laufen.
Der Filter bewegt sich genau gleich wie ein Normaler mit den Unterschied das es keine lernbaren Parameter hat und
die Ausgabe des Filters der grösste Wert von den Eingaben ist (siehe Abbildung~\ref{fig:pool1} ).
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2pt,
  row sep=2pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]|  & |[plain]|  & |[plain]|  & |[plain]| & |[plain]|  & |[plain]|  & |[plain]| \\
&  &   &  &  |[plain]|  & |[plain]|\\
&  &   &  &  |[plain]|  & |[plain]|\\
&  &   &  &  |[plain]|  & |[plain]| &  &  \\
&  &   &  &  |[plain]|  & |[plain]| &  &   \\
};
\node[] at (mat-1-2.east)
  (x) {\centering Feature Map};
\node[] at (mat-2-1)
  (y) {\centering 1};
\node[] at (mat-2-2)
  (y) {\centering 2};
\node[] at (mat-3-2)
  (y) {\centering 3};
\node[] at (mat-3-1)
  (y) {\centering 4};

\node[] at (mat-4-7)
  (y) {\centering 4};

\foreach \ai in {2,...,3}
{\foreach \aii in {1,...,2}
  \node[circle, line width=1pt, draw, inner sep=10pt] at (mat-\ai-\aii) {};
}
\foreach \ai in {2,...,3}
{\foreach \aii in {1,...,2}
  \draw[->] (mat-\ai-\aii) -- (mat-4-7);
}
\end{tikzpicture}
    \caption{2x2 maxpool Schicht mit eine Schrittweite von 2}
    \label{fig:pool1}
\end{figure}
Man kann max pooling verstehen als eine reduzierung der vorhanden Informationen.
Es nimmt das wichtigste Merkmal in einem gewissen Bereich und wirft die weniger wichtigeren Merkmale weg,
so dass es weniger Neuronen gibt und die darauf folgenden Neuronen es einfacher haben.

\subsection{Convolution und Völlig Verbundene Schichten}
Um das Netzwerk zu interpretieren muss man eine völlig verbundenen Schicht am Schluss haben,
da man eine 2-Dimensionale Schicht nicht interpretieren kann.\footnote{Es gibt auch Netzwerke, bei denen man eine convolution Schicht als Ausgabeschicht hat\cite{fullconvolution}}
Diese wird angehängt indem jedes Neuron von der convolution bzw maxpool Schicht mit jedem Neuron der völlig verbundenen Schicht verbunden wird.
Es spielt keine Rolle, dass die Neuronen 2-Dimensional angeordnet sind.

Das trainieren des Netzwerk ist immer noch genau gleich.
Es wird immer noch gradient descent und backpropagation benutzt,
allerdings müssen die Gleichungen der backpropagation für die Convolution und max pooling angepasst werden.

\section{Regularization}
Um ein Netzwerk zu trainieren hat man meisten nur eine endliche Anzahl an Daten an denen das Netzwerk lernen kann.
Aus dem Grund werden die gleichen Daten mehrmals zum trainieren verwendet.
Dadurch kann ein Problem entstehen.
Mit der Zeit kennt das Netzwerk die trainings Daten so gut, dass es die trainings Daten einfach auswendig lernt
und die Daten nicht mehr an ihren gemeinsamen Merkmalen und zusammenhängen erkennt,
sondern an ihren ganz spezifischen Merkmalen die nur für die trainings Daten zutreffen,
so das unbekannte Daten nicht mehr richtig erkannt werden.
Dieses Phänomen ist als \textit{overfitting} bekannt.
\subsection{Dropout}
Beim trainieren eines Netzwerkes werden Neuronen mit einer bestimmten Wahrscheinlichkeit temporär deaktiviert bzw ignoriert (siehe Abbildung~\ref{fig:dropout1}).
Bei jeder neuen Eingabe zum trainieren werden neue zufällige Neuronen ausgewählt,
dabei sind die eingabe und ausgabe Neuronen davon ausgenommen.

\begin{figure}[h]
    \centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]|  & |[plain]|  & |[plain]|   \\
& |[plain]|& |[plain]|  \\
|[plain]| &|[plain]|& |[plain]|  \\
& |[plain]|&& |[plain]| \\
  |[plain]| && |[plain]|& \\
& |[plain]| & \\
  |[plain]| && |[plain]| & \\
& |[plain]| &|[plain]|& |[plain]|  \\
  |[plain]| & |[plain]|& |[plain]|  \\
& |[plain]|& |[plain]|  \\     };
\foreach \ai in {2,4,...,10}
{\foreach \aii in {5, 7}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {5,7}
{\foreach \aii in {4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {4,6}
{\foreach \aii in {5, 7}
  \draw[->] (mat-\ai-3) -- (mat-\aii-4);
}

\node[circle, draw,dotted, inner sep=10pt] at (mat-3-2) {};
\node[circle, draw,dotted, inner sep=10pt] at (mat-9-2) {};
\node[circle, draw,dotted, inner sep=10pt] at (mat-8-3) {};

\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,9}
  \draw[->, dotted] (mat-\ai-1) -- (mat-\aii-2);
  \
}
\foreach \ai in {3,9}
{\foreach \aii in {4,6,8}
  \draw[->, dotted] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {8}
{\foreach \aii in {5, 7}
  \draw[->, dotted] (mat-\ai-3) -- (mat-\aii-4);
}

\end{tikzpicture}
    \caption{neuronales Netzwerk mit Dropout}
    \label{fig:dropout1}
\end{figure}

Wenn das genug oft wiederholt wurde, wurden bestimmte Gewichte und Biases gelernt,
unter der Bedingung das ein Teil der Neuronen deaktiviert sind.
Das heist wenn man das Netzwerk dann benutzt werden mehr Neuronen gleichzeitig Aktiv sein als beim trainieren,
um das Auszugleichen wird die Ausgabe des Neuron mit der Wahrscheinlichkeit, mit der es deaktiviert wird, multipliziert.

Dropout hilft gegen overfitting,
da sich ein Neuron nicht auf bestimmte andere Neuronen verlassen kann,
wodurch es gezwungen ist mit vielen zufälligen Verbindungen etwas nützliches anzufangen.
Anders gesagt das Netzwerk wird robust gegen den Verlust von einzelnen Merkmalen und kann sich nicht auf ein einzelnes Merkmale verlassen.

\subsection{Batch Normalization}
Eine weitere Methode um overfitting zu vermeiden ist die \textit{Batch Normalization}.
Die Eingaben für ein Netzwerk werden normalisiert,
damit die Wahl der learning rate nicht auch noch von den Eingaben abhängig ist.
Batch Normalization führt diese Idee weiter und normalisiert nicht die Eingabe,
sondern die Ausgabe der versteckten Schichten
so dass in den versteckten Schichten extrem hohen Werte vermieden werden.\cite{batchnorm}
Ähnlich wie bei Dropout bringt Batch Normalization eine leichte Störung in das Netzwerk,
welches gegen overfitting hilft\cite{batchnorm}

Die Schichten werden normalisiert indem beim trainieren mehrere Eingaben gleichzeitig durch das Netzwerk laufen und
die Ausgabe eines Neurones mit dem Mittelwert der Eingaben subtrahieren und mit der Standardabweichung der Eingaben dividieren.\cite{batchnorm}
\begin{gather*}
    \mu = \frac{1}{m}\sum^m_{i=1}x_{i}\\
    \sigma = \frac{1}{m}\sum^m_{i=1}(x_i - \mu)^2\\
    \hat{x_i} = \frac{x_i - \mu}{\sqrt{\sigma}}\\
\end{gather*}
wobei $x_i$ die $i^{te}$ Eingabe ist.

Ausserdem wird danach noch mit einem bestimmten Wert $\gamma_i$ multipliziert und einem bestimmten Wert $beta_i$ addiert.
\[y_i = \gamma * \hat{x_i} + \beta \]
wobei $\gamma$ und $\beta$ Parameter sind die beim Netzwerk trainiert werden, wie ein normales Gewicht.\cite{batchnorm}
Die Parameter werden benutzt um dem Netzwerk die Option zu geben die normalisierung zu verändern oder sogar rückgängig zu machen, wenn es meint,
dass es ohne besser funktioniert.\cite{batchnorm_paper}

\chapter{Lösungsansatz}
\label{ch:lösungsansatz}
\section{Logo}
Um das Logo zu erkennen muss man zuerst verstehen wie es auf das Bild gelangt.
Man könnte meinen das Logo wird einfach über das ander Bild gelegt wird,
aber das Logo wird eingespielt indem es mit dem Bild negativ multipliziert wird,
d.h die das Bild und das Logo werden invertiert, multipliziert und dann wieder invertiert.\cite{wiki:blend}
\[f(a,b) = 1 - (1-a)(1-b)\]
wobei $a$ ein Bild ist und $b$ das Logo und die Werte von jedem Pixel von 0 (Schwarz) bis 1 (Weiss) gehen.

Das bewirkt, dass man leicht durch das Logo hindurchsehen kann,
wodurch der Hintergrund hinter dem Logo auch eine Rolle spielt (siehe Abbildung~\ref{fig:logo5}a,b).
\begin{figure}[h]%
    \centering
    \subfloat[]{{\includegraphics[width=3.5cm]{assets/images/logo_beispiel1.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3.5cm]{assets/images/logo_beispiel2.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3.5cm]{assets/images/logo_beispiel3.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3.5cm]{assets/images/logo_beispiel4.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3.5cm]{assets/images/logo_beispiel5.png} }}%
    \qquad
    \subfloat[]{{\includegraphics[width=3.5cm]{assets/images/logo_beispiel6.png} }}%
    \caption{Logo mit verschiedenen Hintergründen}%
    \label{fig:logo5}%
\end{figure}
Eine andere Eingenschaft ist, dass bei manchen Hintergründen,
vor allem bei weissen, das Logo abgeschnitten oder kaum bis gar nicht sichtbar ist (siehe Abbildung~\ref{fig:logo5}c,d,e).
Das ist auch einfach zu erklären.
Wenn man für $a = 1$ (Weiss) in die Formel oben einsetzt erhält man: $f(1, b) = 1$, was bedeutet, dass das ganze Bild nun Weiss ist und das Logo nicht mehr erkennbar ist.
Analog dazu, wenn man für $a = 0$ (Schwarz) einsetzt erhält man: $f(0, b) = b$, was bedeutet, dass das Logo, bei einem schwarzem Hintergrund,
sich im ursprünglichen Zustand befindet.
Aus dem Grund kann man das Logo komplett herausfiltern und wieder verwenden wenn man das Logo mit einem schwarzen Hintergrund findet (siehe Abbildung~\ref{fig:logo5}f).

\section{Werbung erkennen mit Logo}
Um ein Netzwerk zu trainieren braucht man sehr viele Daten, die schon kategorisiert sind.
Eine Option, diese zu beschaffen, wäre die Bilder vom Sender selber zu kategorisiert, das Problem dabei, ist das es eine sehr lange und sehr langweilig arbeit wäre,
da man um die millionen Bilder kategorisieren müsste.
Die andere Möglichkeit ist die Bilder selber zu generieren,
indem man das Logo, auf einen schwarzen Hintergrund, auf viele andere Bilder darauf multipliziert.
Das Logo, auf einen schwarzen Hintergrund, findet man indem man lang genug im Sender darauf wartet.
Die anderen Bilder bekommt man aus dem Open Images Dataset V3\cite{openimages}, welches um die 9 millionen Bilder enthält.
Da diese Bilder des Open Images Dataset nicht in der richtigen Grösse vorhanden sind, werden sie zerteilt in die richtige Grösse.
Die Bilder könnten auch auf die richtige Grösse skaliert werden.
Das Problem dabei ist, dass dadurch viel weniger Bilder pro Sekunden erzeugt werden und das den ganzen Process des Lernen deutlich verlangsamen würde.

Das Logo kann nun mit dem Bild negativ multipliziert, so dass das Logo an einem zufälligen Ort auf dem Bild erscheint (siehe Abbildung~\ref{fig:logo8}).
Man könnte sich auch überlegen, ob man das Logo nur an Position einspielt, wo es auch im Sender vorkommt.
Aber für ein convolution Netzwerk spielt es keine grosse Rolle und der Algorithmus sollte so allgemein wie möglich sein.
\begin{figure}[h]%
    \centering
    \includegraphics[width=6cm]{assets/images/logo_on_random_image.png}%
    \caption{Selbst generiertes Bild mit einem Prosieben Logo}%
    \label{fig:logo8}%
\end{figure}
Die grösse von einem Bild ist 320x180 Pixel, da ein grössers Bild unnötig viele Informationen enthält,
was dazu führt, dass das Netzwerk länger lernen müsste.
Ein kleineres Bild, würde das Logo noch kleiner machen und kaum noch erkennbar (siehe Abbildung~\ref{fig:logo6}).
\begin{figure}[h]%
    \centering
    \includegraphics[width=5cm]{assets/images/logo17x11.png}%
    \caption{Prosieben Logo (17x11 Pixel) herausgenommen aus einem 320x180 Pixel Bild}%
    \label{fig:logo6}%
\end{figure}

Mit diesen selber generierten Bildern kann man ohne Probleme ein neuronales Netzwerk trainieren, welches das Logo erkennen kann.
\section{Werbung erkennen ohne Logo}
Um Werbung zu erkennen ohne ein Logo braucht es die richtige Senderbilder, da Werbebilder nicht generiert werden können.
Da ich die Senderbilder nicht selber kategorisieren will benutze ich das neuronale Netzwerk das mithilfe des Logo die Werbung erkennt, um die Senderbilder zu kategorisieren.
Ein Problem dabei ist, dass das Netzwerk nicht perfekt ist, um das ein bisschen auszugleichen wird die durchschnittliche Vorhersage des Netzwerk genommen,
da Werbung bzw. das normale Programm immer ein weile am Stück läuft, bevor es wechselt

Damit sichergestellt ist, dass das neue Netzwerk nicht wieder das Logo erkennt, sondern die Werbung,
wird das Bild so zugeschnitten, dass das Logo nicht mehr auf dem Bild vorhanden ist.


\chapter{Umsetzung}
\label{ch:umsetzung}
Der ganze Code kann auf Github unter https://github.com/GeorgOhneH/WerbeSkip gefunden werden.
Die verwendete Programmiersprache ist Python 3.
\section{Neuronales Netzwerk}
Der Code für diesen Teil befindet sich im Ordner \textit{deepnet}.
\medskip

Die Grundidee der Implementation für das neuronales Netzwerk ist, dass es Objekt Orientiert ist und man leicht neue Schichten und Funktionen hinzufügen kann.
Die Benutzerschnittstelle ist angelegt an Keras, eine Bibliothek für neuronale Netzwerke.
Als Grundbaustein wird Numpy, ein Bibliothek für wissenschaftliche Datenverarbeitung, benutzt.
Da die Geschwindigkeit ein entscheidender Punkt ist, ist alles was möglich ist als eine matrix multiplikation implementiert.

\subsection{Aufbau}
\paragraph{Schichten}
Jede Type von Schicht ist als eine Klasse implementiert, welche von einer Basisklasse erbt, damit jede Schicht gleich behandelt werden kann.
Jede Schichten implementieren jeweils den vorwärts und den rückwärts (backpropagation) Gangs des Netzwerks
Die Schichten sind unabhängig, bekommen aber immer die Ausgabe der vorigen Schicht, bzw. der hintern Schicht bei der backpropagation.
Die Werte die sie brauchen für die backpropagation, werden intern in jeder Schicht gespeichert.
\paragraph{Kostenfunktionen}
Jede Kostenfunktion ist auch eine Klasse und erbt auch von einer Basisklasse.
Jede Kostenfunktion benötigt die implementation von der Funktion und deren Ableitung.
\paragraph{Optimierer}
Ein Optimierer enthält die Funktionen für des gradient descent, bzw. eine Variante davon,
da es gewisse Varianten des gradient descent gibt die den Process des lernen noch beschleunigen können,
z.B wird noch eine simulierte Beschleunigung in den gradient descent mit einbezogen.\cite{optimization}
Auch die Optimierer Klasse erbt von einer Basisklasse.
Der Optimierer wird an jede Schicht weiter geleitet um die Gewichte und Biases anzupassen.
\paragraph{Netzwerk}
Dies ist die Haputklasse, die alle die Schichten, Kostenfunktionen und Optmierer zusammen setzt und sie sich so untereinander verständigen können
und ist die Benutzerschnittstelle
Ausserdem implementiert die Klasse noch diverse Funktionen, die zur Auswertung des neuronale Netzwerk hilfreich sind.

\subsection{Benutzung}
Beispiel Code kann unter \textit{deepnet/examples} gefunden werden.
\medskip

Um das Programm zu benutzen muss zuerst die eingabe Dimensionen von den Bildern bestimmen werden, welche der eingabe Schicht entspricht.
Bei einer nur völlig verbundenen Netzwerk ist die Anzahl Neuronen, der eingabe Schicht.
Wenn es ein convolution Netzwerk ist, dann besteht die eingabe Dimensionen aus 3 Zahlen.
Die erste Zahl ist die Anzahl feature map, der eingabe Schicht, welche der Anzahl Farbkanäle im Bild entspricht,
die Zweite die Höhe des Bildes und die Dritte die Breite des Bildes.
Danach müssen die verschiedenen Schichten bestimmt werden, darunter sind auch die Aktivierungsfunktionen, da sie auch als eine Schicht implementiert sind.
Als nächstes muss die Kostenfunktion und der Optimierer definiert werden.
Am Schluss muss das neuronale Netzwerk noch trainiert werden, indem die trainings Daten dem Netzwerk gegeben werden.
Die trainings Daten müssen ein numpy array sein mit der gleichen Form, wie die eingabe Dimensionen mit dem Unterschied,
dass der numpy array noch an erster Stelle eine weiter Dimension hat, welcher die Anzahl der Bilder enthält.
Die training Daten können auch als Generator übergeben werden.

\subsection{Grafikkarten unterstützung}
Der Code kann unter \textit{numpywrapper} gefunden werden.
\medskip

Die Geschwindigkeit des Programmes spielt eine entscheidende Rolle für ein
neuronales Netzwerk und die Geschwindigkeit der CPU\footnote{Central processing unit} ist nicht ausreichend.
Um die GPU\footnote{Grafikkarte} zu benutzen, wird Cupy verwendet.
Da Cupy genau die gleichen Funktionen wie Numpy hat, kann es einfach mit Numpy ausgetauscht werden,
Dazu wird ein selbst geschriebenes Module verwendet um einfach zwischen beide hin und her zuschalten.
Die GPU ist schneller als die CPU, da alles mithilfe von matrix Multiplikationen implementiert ist und
die GPU auf matrix Multiplikationen spezialisiert ist.

\subsection{Bild bearbeitung}
Der Code kann unter \textit{helperfunctions/image\_processing} gefunden werden.
\medskip

Der Ordner enthält die Funktionen für die Beschaffung und Formatierung der Bilder, damit sie vom Netzwerk benutzt werden können.
Um die Bilder zu bearbeiten wird OpenCV 2\footnote{Bibliothek von funktionen um Bilder zu bearbeiten} und Numpy verwendet.

Die grösste Herausforderung war die Beschaffung der livestream Bilder von einem Sender.
Um die sender Bilder zu erhalten, wird der Teleboy Stream angezapft, nach einem Beispiel von Github.\cite{gittele} und
wird durch ffmpeg\footnote{Programm für das Aufnehmen, Konvertieren und Streamen von Audio und Video} dekodiert.

\section{Webserver}
Der Code kann unter \textit{src} gefunden werden.
\medskip

Um das fertige Programm auch zu benutzen wird eine Webseite verwendet, die unter der URL \textit{werbeskip.com} erreichbar ist
Für das Backend des Servers wird Django\footnote{Ein web framework} und Django Channels verwendet, wodurch ein Websocket benutzt werden kann,
so dass die Verbindung mit dem Server offen bleibt und die Seite immer auf dem aktuellen Stand ist..

Für das Frontend wird VueJs und VuetifyJS verwendet und es ist als eine Single Page Application implementiert.

\chapter{Ergebnisse}
\section{Auswertung}
\subsection{Datensatz}
Um die Leistung der Netzwerke richtig zu Bewerten, wird ein selbst erstellter Datensatz verwendet,
welches aus insgesamt 7830 Prosieben Bildern besteht.
Aus den 7830 Bildern sind 4495 Bilder mit Logo und ohne Rand, 813 mit Logo und einem oberen und untern schwarzen Rand,
813 mit Logo und einem schwarzen Rand auf beiden Seiten und 2095 Bilder ohne Logo und ohne Rand.
Ausserdem enthält der Datensatz noch 400 Bilder, die nicht das klassische Prosieben Logo enthält (siehe Abbildung~\ref{fig:logo_spec}),
welche exkludiert sind vom Datensatz.
\begin{figure}[h]%
    \centering
    \includegraphics[width=7cm]{assets/images/logo_special.png}%
    \caption{Kein klassisches Prosieben Logo}%
    \label{fig:logo_spec}%
\end{figure}
\subsection{Methoden}
\paragraph{loss} Die einfachste Methode das Netzwerk auszuwerten ist der Wert der Kostenfunktion anzuschauen, welcher als \textit{loss} bezeichnet wird.
So tiefer der loss ist umso besser besser ist das Netzwerk.

\paragraph{Genauigkeit} Eine andere relativ einfache Methode ist die Genauigkeit des Netzwerks zu bestimmen,
indem man die richtig erkannten Bilder durch die totale Anzahl Bilder teilt.
Das Problem bei der Genauigkeit ist, dass sie nicht sehr Aussagekräftig ist,
wenn das Datensatz nicht ausgeglichen ist.
Zum Beispiel hat man ein Datensatz von 100 Bildern, 90 von den Bildern haben ein Logo und 10 haben keins.
Wenn jetzt ein neuronales Netzwerk immer sagt, dass ein Logo auf dem Bild ist, ergebe das eine Genauigkeit von 90\%,
was nach einem gutes Ergebnis ausschaut, aber das Netzwerk ist im Grunde nutzlos, da es das Logo nicht erkennt,
sondern immer die gleiche Ausgabe ergibt.

\paragraph{Matthews correlation coefficient} Der Matthews correlation coefficient\cite{wiki:mcc} behebt genau dieses Problem.
MCC\footnote{Matthews correlation coefficient} unterscheidet nicht nur zwischen Falschen und Wahren Vorhersagen des Netzwerk,
sondern unterscheidet auch zwischen wahr positiv, wahr negativ, falsch positiv und falsch negativ (Siehe Tabelle~\ref{table:mcc}).
\begin{table}[h]
    \centering
\begin{tabular}{cc|c|c|}
\cline{3-4}
                                                                                                        &                                                                           & \multicolumn{2}{c|}{Wahrer Zustand} \\ \cline{3-4}
                                                                                                        &                                                                           & Zustand positiv   & Zustand negativ \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Vorausgesagt\\ Bedingung\end{tabular}}} & \begin{tabular}[c]{@{}c@{}}Vorhergesagter\\  Zustand positiv\end{tabular} & wahr positiv  & falsch positiv  \\ \cline{2-4}
\multicolumn{1}{|c|}{}                                                                                  & \begin{tabular}[c]{@{}c@{}}Vorhergesagter\\ Zustand negativ\end{tabular}  & falsch negativ    & wahr negativ   \\ \hline
\end{tabular}

\caption{My table}
\label{table:mcc}
\end{table}
MCC gibt einen Wert von -1 bis +1 zurück.
Der Wert +1 repräsentiert eine perfekte Vorhersage,
0 nicht besser als eine zufällige und -1 eine komplette Unstimmigkeit zwischen Netzwerk und dem Datensatz.
Der MCC wird nach folgender Formel berechnet\cite{wiki:mcc}:
\[MCC = \frac{WP \cdot WN - FP \cdot FN}{\sqrt{(WP+FP)(WP+FN)(WN+FP)(WN+FN)}}\]
wobei WP die Anzahl der wahr positiven ist, WN die Anzahl der wahr negativen ist,
FP die Anzahl der falsch positiven ist und FN die Anzahl der falsch negativen ist.

\section{Neuronales Netzwerk mithilfe des Logo's}
\subsection{Netzwerk Architektur}
\begin{itemize}
    \setlength\itemsep{0cm}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item  Convolution, Filter: 16, Filterbreite: 12, Filterhöhe: 8, Schrittweite 4
    \item BatchNorm
    \item ReLU
    \item Convolution, Filter: 64, Filterbreite: 6, Filterhöhe: 4, Schrittweite 1, Neuronenrand 2
    \item BatchNorm
    \item ReLU
    \item Maxpool, Filterbreite: 3 Filterhöhe: 3, Schrittweite 1
    \item Convolution, Filter: 128, Filterbreite: 4, Filterhöhe: 4, Schrittweite 1
    \item BatchNorm
    \item ReLU
    \item Convolution, Filter: 128, Filterbreite: 3, Filterhöhe: 3, Schrittweite 2
    \item BatchNorm
    \item ReLU
    \item Convolution, Filter: 256, Filterbreite: 3, Filterhöhe: 3, Schrittweite 1
    \item BatchNorm
    \item Dropout 25\%
    \item Völlig verbundene Schicht, Neuronen: 512
    \item BatchNorm
    \item ReLU
    \item Dropout 50\%
    \item Völlig verbundene Schicht, Neuronen: 2
    \item Softmax
\end{itemize}

\chapter{Reflexion}
\label{ch:reflexion}

\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Literaturverzeichnis}
\nocite{*}
\bibliography{C:/Jetbrains/PyCharm/WerbeSkip/thoughts/arbeit/maturaarbeit}
\bibliographystyle{plain}

\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\listoffigures
Abbildung aus dem Kapitel~\ref{ch:neuronalesNetzwerk} sind von Michael A. Nielsen's Buch\cite{neuralbook} inspiriert.

\appendix


\chapter*{Ehrlichkeitserklärung}

Die eingereichte Arbeit ist das Resultat meiner persönlichen, selbstständigen Beschäftigung mit dem Thema.
Ich habe für sie keine anderen Quellen benutzt als die in den Verzeichnissen aufgeführten.
Sämtliche wörtlich übernommenen Texte (Sätze) sind als Zitate gekennzeichnet.

\vspace{2cm}
\today
\end{document}
